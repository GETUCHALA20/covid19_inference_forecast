{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import matplotlib\n",
    "import pickle\n",
    "try:\n",
    "    from fastprogress.fastprogress import progress_bar as tqdm\n",
    "except:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "confirmed_cases_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n",
    "confirmed_cases = pd.read_csv(confirmed_cases_url, sep=',')\n",
    "deaths_url =  'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv'\n",
    "deaths = pd.read_csv(deaths_url, sep=',')\n",
    "path_to_save = '../../figures/'\n",
    "path_data = '../../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delay_cases(new_I_t, len_new_I_t, len_new_cases_obs , delay, delay_arr):\n",
    "    \"\"\"\n",
    "    Delays the input new_I_t by delay and return and array with length len_new_cases_obs\n",
    "    The initial delay of the output is set by delay_arr. \n",
    "    \n",
    "    Take care that delay is smaller or equal than delay_arr, otherwise zeros are \n",
    "    returned, which could potentially lead to errors\n",
    "\n",
    "    Also assure that len_new_I_t is larger then len(new_cases_obs)-delay, otherwise it \n",
    "    means that the simulated data is not long enough to be fitted to the data.\n",
    "    \"\"\"\n",
    "    delay_mat = make_delay_matrix(n_rows=len_new_I_t, \n",
    "                                  n_columns=len_new_cases_obs, initial_delay=delay_arr)\n",
    "    inferred_cases = interpolate(new_I_t, delay, delay_mat)\n",
    "    return inferred_cases \n",
    "\n",
    "def make_delay_matrix(n_rows, n_columns, initial_delay=0):\n",
    "    \"\"\"\n",
    "    Has in each entry the delay between the input with size n_rows and the output\n",
    "    with size n_columns\n",
    "    \"\"\"\n",
    "    size = max(n_rows, n_columns)\n",
    "    mat = np.zeros((size, size))\n",
    "    for i in range(size):\n",
    "        diagonal = np.ones(size-i)*(initial_delay + i)\n",
    "        mat += np.diag(diagonal, i)\n",
    "    for i in range(1, size):\n",
    "        diagonal = np.ones(size-i)*(initial_delay - i)\n",
    "        mat += np.diag(diagonal, -i)\n",
    "    return mat[:n_rows, :n_columns]\n",
    "\n",
    "def interpolate(array, delay, delay_matrix):\n",
    "    interp_matrix = tt.maximum(1-tt.abs_(delay_matrix - delay), 0)\n",
    "    interpolation = tt.dot(array,interp_matrix)\n",
    "    return interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cases yesterday (2020-03-15): 5795 and day before yesterday: 4585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [σ_obs, delay, μ, λ, I_begin]\n",
      "Sampling 4 chains, 44 divergences: 100%|██████████| 20000/20000 [01:41<00:00, 196.73draws/s]\n",
      "There were 11 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 14 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 6 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 13 divergences after tuning. Increase `target_accept` or reparameterize.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model run in 117.97 s\n"
     ]
    }
   ],
   "source": [
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "import theano\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "date_data_begin = datetime.date(2020,3,1)\n",
    "date_data_end = datetime.date(2020,3,15)\n",
    "num_days_to_predict = 28\n",
    "\n",
    "\n",
    "diff_data_sim = 16 # should be significantly larger than the expected delay, in \n",
    "                   # order to always fit the same number of data points.\n",
    "date_begin_sim = date_data_begin - datetime.timedelta(days = diff_data_sim)\n",
    "format_date = lambda date_py: '{}/{}/{}'.format(date_py.month, date_py.day,\n",
    "                                                 str(date_py.year)[2:4])\n",
    "date_formatted_begin = format_date(date_data_begin)\n",
    "date_formatted_end = format_date(date_data_end)\n",
    "\n",
    "cases_obs =  np.array(\n",
    "    confirmed_cases.loc[confirmed_cases[\"Country/Region\"] == \"Germany\", \n",
    "                        date_formatted_begin:date_formatted_end])[0]\n",
    "#cases_obs = np.concatenate([np.nan*np.ones(diff_data_sim), cases_obs])\n",
    "print('Cases yesterday ({}): {} and '\n",
    "      'day before yesterday: {}'.format(date_data_end.isoformat(), *cases_obs[:-3:-1]))\n",
    "num_days = (date_data_end - date_begin_sim).days\n",
    "date_today = date_data_end + datetime.timedelta(days=1)\n",
    "# ------------------------------------------------------------------------------ #\n",
    "# model setup and training\n",
    "# ------------------------------------------------------------------------------ #\n",
    "np.random.seed(0)\n",
    "\n",
    "def SIR_model(λ, μ, S_begin, I_begin, N):\n",
    "    new_I_0 = tt.zeros_like(I_begin)\n",
    "    def next_day(λ, S_t, I_t, _):\n",
    "        new_I_t = λ/N*I_t*S_t\n",
    "        S_t = S_t - new_I_t\n",
    "        I_t = I_t + new_I_t - μ * I_t\n",
    "        return S_t, I_t, new_I_t\n",
    "    outputs , _  = theano.scan(fn=next_day, sequences=[λ], \n",
    "                               outputs_info=[S_begin, I_begin, new_I_0])\n",
    "    S_all, I_all, new_I_all = outputs\n",
    "    return S_all, I_all, new_I_all\n",
    "\n",
    "with pm.Model() as model:\n",
    "    # true cases at begin of loaded data but we do not know the real number\n",
    "    I_begin = pm.HalfCauchy('I_begin', beta=100)\n",
    "\n",
    "    # fraction of people that are newly infected each day\n",
    "    λ = pm.Lognormal(\"λ\", mu=np.log(0.4), sigma=0.5)\n",
    "\n",
    "    # fraction of people that recover each day, recovery rate mu\n",
    "    μ = pm.Lognormal('μ', mu=np.log(1/8), sigma=0.2)\n",
    "\n",
    "    # delay in days between contracting the disease and being recorded\n",
    "    delay = pm.Lognormal(\"delay\", mu=np.log(8), sigma=0.2)\n",
    "\n",
    "    # prior of the error of observed cases\n",
    "    σ_obs = pm.HalfCauchy(\"σ_obs\", beta=10)\n",
    "\n",
    "    N_germany = 83e6\n",
    "\n",
    "    # -------------------------------------------------------------------------- #\n",
    "    # training the model with loaded data\n",
    "    # -------------------------------------------------------------------------- #\n",
    "\n",
    "    S_begin = N_germany - I_begin\n",
    "    S_past, I_past, new_I_past = SIR_model(λ=λ * tt.ones(num_days-1), μ=μ, \n",
    "                                               S_begin=S_begin, I_begin=I_begin,\n",
    "                                               N=N_germany)\n",
    "    new_cases_obs = np.diff(cases_obs)\n",
    "    new_cases_inferred = delay_cases(new_I_past, len_new_I_t=num_days - 1, \n",
    "                                     len_new_cases_obs=len(new_cases_obs), \n",
    "                                     delay=delay, delay_arr=diff_data_sim)\n",
    "\n",
    "    # Approximates Poisson\n",
    "    # calculate the likelihood of the model:\n",
    "    # observed cases are distributed following studentT around the model\n",
    "    pm.StudentT(\n",
    "        \"obs\",\n",
    "        nu=4,\n",
    "        mu=new_cases_inferred,\n",
    "        sigma=(new_cases_inferred)**0.5 * σ_obs,\n",
    "        observed=new_cases_obs)  \n",
    "    \n",
    "    S_past = pm.Deterministic('S_past', S_past)\n",
    "    I_past = pm.Deterministic('I_past', I_past)\n",
    "    new_I_past = pm.Deterministic('new_I_past', new_I_past)\n",
    "    new_cases_past = pm.Deterministic('new_cases_past', new_cases_inferred)\n",
    "    \n",
    "\n",
    "    # -------------------------------------------------------------------------- #\n",
    "    # run model, pm trains and predicts when calling this\n",
    "    # -------------------------------------------------------------------------- #\n",
    "    \n",
    "    time_beg = time.time()\n",
    "    trace = pm.sample(draws=3000, tune=2000, chains=4)\n",
    "    print(\"Model run in {:.2f} s\".format(time.time() - time_beg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model_flat:\n",
    "    # true cases at begin of loaded data but we do not know the real number\n",
    "    I_begin = pm.Flat('I_begin')\n",
    "\n",
    "    # fraction of people that are newly infected each day\n",
    "    λ = pm.Flat(\"λ\")\n",
    "\n",
    "    # fraction of people that recover each day, recovery rate mu\n",
    "    μ = pm.Flat('μ')\n",
    "\n",
    "    # delay in days between contracting the disease and being recorded\n",
    "    delay = pm.Flat(\"delay\")\n",
    "\n",
    "    # prior of the error of observed cases\n",
    "    σ_obs = pm.Flat(\"σ_obs\")\n",
    "\n",
    "    N_germany = 83e6\n",
    "\n",
    "    # -------------------------------------------------------------------------- #\n",
    "    # training the model with loaded data\n",
    "    # -------------------------------------------------------------------------- #\n",
    "\n",
    "    S_begin = N_germany - I_begin\n",
    "    S_past, I_past, new_I_past = SIR_model(λ=λ * tt.ones(num_days-1), μ=μ, \n",
    "                                               S_begin=S_begin, I_begin=I_begin,\n",
    "                                               N=N_germany)\n",
    "    new_cases_obs = np.diff(cases_obs)\n",
    "    new_cases_inferred = delay_cases(new_I_past, len_new_I_t=num_days - 1, \n",
    "                                     len_new_cases_obs=len(new_cases_obs), \n",
    "                                     delay=delay, delay_arr=diff_data_sim)\n",
    "\n",
    "    # Approximates Poisson\n",
    "    # calculate the likelihood of the model:\n",
    "    # observed cases are distributed following studentT around the model\n",
    "    pm.StudentT(\n",
    "        \"obs\",\n",
    "        nu=4,\n",
    "        mu=new_cases_inferred,\n",
    "        sigma=(new_cases_inferred)**0.5 * σ_obs,\n",
    "        observed=new_cases_obs)  \n",
    "    \n",
    "    S_past = pm.Deterministic('S_past', S_past)\n",
    "    I_past = pm.Deterministic('I_past', I_past)\n",
    "    new_I_past = pm.Deterministic('new_I_past', new_I_past)\n",
    "    new_cases_past = pm.Deterministic('new_cases_past', new_cases_inferred)\n",
    "    \n",
    "\n",
    "    # -------------------------------------------------------------------------- #\n",
    "    # run model, pm trains and predicts when calling this\n",
    "    # -------------------------------------------------------------------------- #\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I_begin_log__, λ_log__, μ_log__, delay_log__, σ_obs_log__]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186b990d13024a31af07437f9adc7cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=27000000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# brute force search, create log like matrix\n",
    "\n",
    "load_from_file = False\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "from itertools import product\n",
    "from tqdm.notebook import tqdm\n",
    "fn = model.fn(model.logpt)\n",
    "\n",
    "print(model.vars)\n",
    "\n",
    "\n",
    "I_begin_r = np.linspace(np.log(0.5), np.log(500), 50)\n",
    "lambda_r = np.linspace(np.log(0.1), np.log(0.8), 40)\n",
    "mu_r = np.linspace(np.log(1 / 30), np.log(1 / 4), 30)\n",
    "delay_r = np.linspace(np.log(2), np.log(15), 30)\n",
    "sigma_obs_r = np.linspace(np.log(0.5), np.log(60), 20)\n",
    "\n",
    "\n",
    "if not load_from_file:\n",
    "    logp = []\n",
    "    for values in tqdm(product(I_begin_r, lambda_r, mu_r, delay_r, sigma_obs_r), total = 50*40*30*30*15):\n",
    "        point = {\"I_begin_log__\" : values[0], \"λ_log__\": values[1], \"μ_log__\": values[2], \n",
    "                            \"delay_log__\": values[3], \"σ_obs_log__\": values[4]}\n",
    "        logp.append(fn(point))\n",
    "    logp_mat = np.array(logp).reshape((50,40,30,30,20))\n",
    "    with open('logp_mat.pickled', 'wb') as file:\n",
    "        pickle.dump(logp_mat, file)\n",
    "else:\n",
    "    logp_mat = pickle.load(open('logp_mat.pickled', 'rb'))\n",
    "                \n",
    "\n",
    "par_titles = [\"  I_0\", \"    λ\", \"    μ\", \"delay\", \"σ_obs\"]\n",
    "\n",
    "par_values = []\n",
    "par_values.append( np.exp(I_begin_r) )\n",
    "par_values.append( np.exp(lambda_r) )\n",
    "par_values.append( np.exp(mu_r) )\n",
    "par_values.append( np.exp(delay_r) )\n",
    "par_values.append( np.exp(sigma_obs_r) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_flat = model_flat.fn(model_flat.logpt)\n",
    "load_from_file = False\n",
    "if not load_from_file:\n",
    "    logp_flat = []\n",
    "    for values in tqdm(product(np.exp(I_begin_r), \n",
    "                               np.exp(lambda_r), \n",
    "                               np.exp(mu_r), \n",
    "                               np.exp(delay_r), \n",
    "                               np.exp(sigma_obs_r)), total = 50*40*30*30*15):\n",
    "        point = {\"I_begin\" : values[0], \"λ\": values[1], \"μ\": values[2], \n",
    "                            \"delay\": values[3], \"σ_obs\": values[4]}\n",
    "        logp_flat.append(fn_flat(point))\n",
    "    logp_flat_mat = np.array(logp_flat).reshape((50,40,30,30,20))\n",
    "    with open('logp_flat.pickle', 'wb') as file:\n",
    "        pickle.dump(logp_flat_mat, file)\n",
    "else:\n",
    "    with open('logp_flat.pickle', 'rb') as file:\n",
    "        logp_flat_mat = pickle.dump(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get percentiles from bayesian model run\n",
    "\n",
    "# print(trace.varnames)\n",
    "percentiles = np.ones((5,2))*np.nan\n",
    "for idx, key in enumerate([\"I_begin\", \"λ\", \"μ\", \"delay\", \"σ_obs\"]):\n",
    "    percentiles[idx, 0] = np.percentile(trace[key], q=2.5, axis=0)\n",
    "    percentiles[idx, 1] = np.percentile(trace[key], q=97.5, axis=0)\n",
    "\n",
    "print(\"percentiles:\", percentiles)\n",
    "\n",
    "logp_reference = np.max(trace.model_logp)\n",
    "print(f\"reference logp: {logp_reference:.4f}\")\n",
    "\n",
    "\n",
    "# get parameter combinations that are not within 95% ci of bayesian estimate but produce a decent likeliehood\n",
    "special = []\n",
    "for i in tqdm(range(0, len(logp))):\n",
    "    idx = np.unravel_index(i, (30, 40, 30, 30, 20))\n",
    "    this_logp = logp_mat[idx]\n",
    "    \n",
    "    if this_logp < logp_reference-3:\n",
    "        continue\n",
    "    \n",
    "    for par in range(0,5):\n",
    "        if (par_values[par][idx[par]] <= percentiles[par, 0] or\n",
    "            par_values[par][idx[par]] >= percentiles[par, 1]):\n",
    "            special.append(idx)\n",
    "            \n",
    "print(f\"{len(special)} parameters resulted in log likeliehood above {logp_reference-3:.3f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the special guys\n",
    "\n",
    "def is_within_ci(par, pos):\n",
    "    if (par_values[par][pos] <= percentiles[par, 0] or\n",
    "        par_values[par][pos] >= percentiles[par, 1]):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def print_par_from_pos(idx):\n",
    "    for par in range(0,5):\n",
    "        pos = idx[par]\n",
    "        print(f\" {'! ' if not is_within_ci(par, pos) else '  '} {par_titles[par]}: {par_values[par][pos]:.3f}\")\n",
    "    print(\"----------------\")\n",
    "    \n",
    "special_par = []\n",
    "super_special = []\n",
    "print_all = False\n",
    "\n",
    "for this_special in special:\n",
    "    num = 0\n",
    "    if print_all:\n",
    "        print_par_from_pos(this_special)\n",
    "        \n",
    "    for par in range(0,5):\n",
    "        pos = this_special[par]\n",
    "        if not is_within_ci(par, pos):\n",
    "            num += 1\n",
    "            special_par.append(par)\n",
    "            if num > 1:\n",
    "                super_special.append(this_special)\n",
    "\n",
    "                \n",
    "# how many outliers belong to which parameter\n",
    "y = np.bincount(special_par)\n",
    "for par in range(0,5):\n",
    "    print(f\"{par_titles[par]} {y[par]}  (CI: [{percentiles[par, 0]:.2f}, {percentiles[par, 1]:.2f}] Range: [{par_values[par][0]:.2f}, {par_values[par][-1]:.2f}])\")\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# when more than two parameters were out, print them\n",
    "for this_special in super_special:\n",
    "    print_par_from_pos(this_special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxi = np.max(logp_mat, axis=(0,3,4))\n",
    "print(maxi.shape)\n",
    "fig, ax  = plt.subplots(1)\n",
    "cntr1 = ax.contour(np.exp(lambda_r), np.exp(mu_r),maxi.T, levels=20)\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "ax.set_xlabel('infection rate λ')\n",
    "ax.set_ylabel('recovery rate μ')\n",
    "ax.plot(trace.λ, trace.μ, '.', alpha=0.02)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "fig.colorbar(cntr1, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage\n",
    "\n",
    "fig, axes = plt.subplots(2,4, figsize=(10,4), constrained_layout=True)\n",
    "\n",
    "ax = axes[0,0]\n",
    "likeli = np.max(logp_mat, axis=(0,3,4))\n",
    "max_likeli_int = round(np.ceil(np.max(likeli)))\n",
    "\n",
    "cntr1 = ax.contour(np.exp(lambda_r), np.exp(mu_r),likeli.T, \n",
    "                   levels=np.arange(max_likeli_int-60, max_likeli_int+1, 2))\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "ax.set_ylabel('Recovery rate $\\mu$')\n",
    "ax.plot(trace.λ, trace.μ, '.', alpha=0.05, ms=3, mew=0)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "ax = axes[0][1]\n",
    "mat_lambda_mu = np.exp(lambda_r[:, None]) - np.exp(mu_r[None,:])\n",
    "sort = np.argsort(mat_lambda_mu, axis=None)\n",
    "eff_lambda = np.sort(mat_lambda_mu, axis=None)\n",
    "likeli = np.max(logp_mat.reshape(logp_mat.shape[0], -1, *logp_mat.shape[-2:])[:,sort], axis=(0,3))\n",
    "#likeli = scipy.ndimage.maximum_filter(likeli, size=(40,1))\n",
    "likeli = np.max(likeli.reshape(40, 30, 30), axis=1)\n",
    "max_likeli_int = round(np.ceil(np.max(likeli)))\n",
    "cntr2 = ax.contour(eff_lambda[15::30], np.exp(delay_r),likeli.T, \n",
    "                   levels=np.arange(max_likeli_int-60, max_likeli_int+1, 2))\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "ax.set_ylabel('Delay (days)')\n",
    "ax.plot(trace.λ- trace.μ, trace.delay, '.', mew=0, ms=3, alpha=0.05)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "\n",
    "ax = axes[0][2]\n",
    "mat_lambda_mu = np.exp(lambda_r[:, None]) - np.exp(mu_r[None,:])\n",
    "sort = np.argsort(mat_lambda_mu, axis=None)\n",
    "eff_lambda = np.sort(mat_lambda_mu, axis=None)\n",
    "likeli = np.max(logp_mat.reshape(logp_mat.shape[0], -1, *logp_mat.shape[-2:])[:,sort], axis=(2,3))\n",
    "likeli = np.max(likeli.reshape(50, 40, 30), axis=2)\n",
    "max_likeli_int = round(np.ceil(np.max(likeli)))\n",
    "cntr2 = ax.contour(eff_lambda[15::30], np.exp(I_begin_r),likeli, \n",
    "                   levels=np.arange(max_likeli_int-60, max_likeli_int+1, 2))\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel('Initial infections $I_0$')\n",
    "ax.plot(trace.λ- trace.μ, trace.I_begin, '.', mew=0, ms=3, alpha=0.05)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "\n",
    "ax = axes[0][3]\n",
    "mat_lambda_mu = np.exp(lambda_r[:, None]) - np.exp(mu_r[None,:])\n",
    "sort = np.argsort(mat_lambda_mu, axis=None)\n",
    "eff_lambda = np.sort(mat_lambda_mu, axis=None)\n",
    "likeli = np.max(logp_mat.reshape(logp_mat.shape[0], -1, *logp_mat.shape[-2:])[:,sort], axis=(0,2))\n",
    "likeli = np.max(likeli.reshape(40, 30, 20), axis=1)\n",
    "max_likeli_int = round(np.ceil(np.max(likeli)))\n",
    "cntr2 = ax.contour(eff_lambda[15::30], np.exp(sigma_obs_r),likeli.T, \n",
    "                   levels=np.arange(max_likeli_int-60, max_likeli_int+1, 2))\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "#ax.set_yscale('log')\n",
    "ax.set_ylabel('Scale $\\sigma$ of the likelihood')\n",
    "ax.plot(trace.λ- trace.μ, trace.σ_obs, '.', mew=0, ms=3, alpha=0.05)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "cbar = fig.colorbar(cntr2, ax=ax)\n",
    "cbar.set_label('Likelihood\\ninformed priors')\n",
    "\n",
    "\n",
    "ax = axes[1][0]\n",
    "\n",
    "likeli = np.max(logp_flat_mat, axis=(0,3,4))\n",
    "max_likeli_int = round(np.ceil(np.max(likeli)))\n",
    "\n",
    "cntr1 = ax.contour(np.exp(lambda_r), np.exp(mu_r),likeli.T, \n",
    "                   levels=np.arange(max_likeli_int-50, max_likeli_int+2, 2))\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "ax.set_xlabel('Infection rate $\\lambda$')\n",
    "ax.set_ylabel('Recovery rate $\\mu$')\n",
    "ax.plot(trace.λ, trace.μ, '.',  mew=0, ms=3, alpha=0.05)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "ax = axes[1][1]\n",
    "\n",
    "\n",
    "mat_lambda_mu = np.exp(lambda_r[:, None]) - np.exp(mu_r[None,:])\n",
    "sort = np.argsort(mat_lambda_mu, axis=None)\n",
    "eff_lambda = np.sort(mat_lambda_mu, axis=None)\n",
    "likeli = np.max(logp_flat_mat.reshape(logp_mat.shape[0], -1, *logp_mat.shape[-2:])[:,sort], axis=(0,3))\n",
    "#likeli = scipy.ndimage.maximum_filter(likeli, size=(40,1))\n",
    "likeli = np.max(likeli.reshape(40, 30, 30), axis=1)\n",
    "max_likeli_int = round(np.ceil(np.max(likeli)))\n",
    "cntr4 = ax.contour(eff_lambda[15::30], np.exp(delay_r),likeli.T, \n",
    "                   levels=np.arange(max_likeli_int-50, max_likeli_int+2, 2))\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "ax.set_xlabel('Eff. growth rate $\\lambda - \\mu$')\n",
    "ax.set_ylabel('Delay (days)')\n",
    "ax.plot(trace.λ- trace.μ, trace.delay, '.', mew=0, ms=3, alpha=0.05)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "ax = axes[1][2]\n",
    "mat_lambda_mu = np.exp(lambda_r[:, None]) - np.exp(mu_r[None,:])\n",
    "sort = np.argsort(mat_lambda_mu, axis=None)\n",
    "eff_lambda = np.sort(mat_lambda_mu, axis=None)\n",
    "likeli = np.max(logp_flat_mat.reshape(logp_mat.shape[0], -1, *logp_mat.shape[-2:])[:,sort], axis=(2,3))\n",
    "likeli = np.max(likeli.reshape(50, 40, 30), axis=2)\n",
    "max_likeli_int = round(np.ceil(np.max(likeli)))\n",
    "cntr2 = ax.contour(eff_lambda[15::30], np.exp(I_begin_r),likeli, \n",
    "                   levels=np.arange(max_likeli_int-60, max_likeli_int+1, 2))\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "ax.set_ylabel('Initial infections $I_0$')\n",
    "ax.plot(trace.λ- trace.μ, trace.I_begin, '.', mew=0, ms=3, alpha=0.05)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Eff. growth rate $\\lambda - \\mu$')\n",
    "\n",
    "\n",
    "ax = axes[1][3]\n",
    "mat_lambda_mu = np.exp(lambda_r[:, None]) - np.exp(mu_r[None,:])\n",
    "sort = np.argsort(mat_lambda_mu, axis=None)\n",
    "eff_lambda = np.sort(mat_lambda_mu, axis=None)\n",
    "likeli = np.max(logp_flat_mat.reshape(logp_mat.shape[0], -1, *logp_mat.shape[-2:])[:,sort], axis=(0,2))\n",
    "likeli = np.max(likeli.reshape(40, 30, 20), axis=1)\n",
    "max_likeli_int = round(np.ceil(np.max(likeli)))\n",
    "cntr2 = ax.contour(eff_lambda[15::30], np.exp(sigma_obs_r),likeli.T, \n",
    "                   levels=np.arange(max_likeli_int-60, max_likeli_int+1, 2))\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel('Scale $\\sigma$ of the likelihood')\n",
    "ax.plot(trace.λ- trace.μ, trace.σ_obs, '.', mew=0, ms=3, alpha=0.05)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "cbar = fig.colorbar(cntr2, ax=ax)\n",
    "cbar.set_label('Likelihood\\nflat priors')\n",
    "ax.set_xlabel('Eff. growth rate $\\lambda - \\mu$')\n",
    "\n",
    "\n",
    "path_to_save = '../../figures/'\n",
    "plt.savefig(path_to_save + 'Fig_brute_force_contours.png', dpi = 300)\n",
    "plt.savefig(path_to_save + 'Fig_brute_force_contours.svg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likeli = np.max(logp_flat_mat, axis=(0,3,4))\n",
    "fig, ax  = plt.subplots(1)\n",
    "max_likeli_int = round(np.ceil(np.max(likeli)))\n",
    "\n",
    "cntr1 = ax.contour(np.exp(lambda_r), np.exp(mu_r),likeli.T, \n",
    "                   levels=np.arange(max_likeli_int-100, max_likeli_int+2, 2))\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "ax.set_xlabel('infection rate λ')\n",
    "ax.set_ylabel('recovery rate μ')\n",
    "ax.plot(trace.λ, trace.μ, '.', mew=0, alpha=0.05)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "fig.colorbar(cntr1, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage\n",
    "\n",
    "mat_lambda_mu = np.exp(lambda_r[:, None]) - np.exp(mu_r[None,:])\n",
    "sort = np.argsort(mat_lambda_mu, axis=None)\n",
    "eff_lambda = np.sort(mat_lambda_mu, axis=None)\n",
    "likeli = np.max(logp_flat_mat.reshape(logp_mat.shape[0], -1, *logp_mat.shape[-2:])[:,sort], axis=(0,3))\n",
    "#likeli = scipy.ndimage.maximum_filter(likeli, size=(40,1))\n",
    "likeli = np.max(likeli.reshape(40, 30, 30), axis=1)\n",
    "fig, ax  = plt.subplots(1)\n",
    "max_likeli_int = round(np.ceil(np.max(likeli)))\n",
    "cntr1 = ax.contour(eff_lambda[15::30], np.exp(delay_r),likeli.T, \n",
    "                   levels=np.arange(max_likeli_int-100, max_likeli_int+2, 2))\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "ax.set_xlabel('effective growth rate λ - μ')\n",
    "ax.set_ylabel('delay (days)')\n",
    "ax.plot(trace.λ- trace.μ, trace.delay, '.', mew=0, alpha=0.05)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "fig.colorbar(cntr1, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
